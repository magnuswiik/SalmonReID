{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salmon Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os, json\n",
    "import numpy as np\n",
    "from torchvision.io import read_image\n",
    "from torchvision import tv_tensors\n",
    "\n",
    "class SalmonDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        \n",
    "        self.imgs = [file for file in sorted(os.listdir(os.path.join(root, \"Images\"))) if file.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        self.annots = [file for file in sorted(os.listdir(os.path.join(root, \"Boxes\"))) if file.endswith('.json')]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        img_path = os.path.join(self.root, \"Images\", self.imgs[idx])\n",
    "        annots_path = os.path.join(self.root, \"Boxes\", self.annots[idx])\n",
    "        \n",
    "        img = read_image(img_path)\n",
    "        if \"DS\" not in annots_path:\n",
    "            try:\n",
    "                shapes = json.load(open(annots_path))['shapes']\n",
    "            except:\n",
    "                print(\"Cannot open json file...\")\n",
    "            \n",
    "        boxes = [shape['points'] for shape in shapes]\n",
    "        boxes_flattened = []\n",
    "        for box in boxes:\n",
    "            box = np.array(box).flatten().tolist()\n",
    "            boxes_flattened.append(box)\n",
    "        \n",
    "        boxes_flattened = torch.tensor(boxes_flattened, dtype=torch.float)\n",
    "        \n",
    "\n",
    "        # Number of boxes in image\n",
    "        num_objs = len(boxes_flattened)\n",
    "\n",
    "        # there is only one class -> Salmon = 1\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        # Wrap sample and targets into torchvision tv_tensors:\n",
    "        img = tv_tensors.Image(img)\n",
    "        img = img.float() / 255.0\n",
    "        \n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes_flattened\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = idx\n",
    "        target[\"area\"] = (boxes_flattened[:,2]-boxes_flattened[:,0])*(boxes_flattened[:,3]-boxes_flattened[:,1])\n",
    "        target[\"iscrowd\"] = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "            \n",
    "        if target[\"boxes\"].numel() == 0:\n",
    "            print(\"No boxes :(\")\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Faster RCNN model pretrained on COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights\n",
    "\n",
    "def get_detection_model(num_classes, weights=FasterRCNN_ResNet50_FPN_Weights):\n",
    "    # load a model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=weights)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training, Validation and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Random seeds for reproducibility\n",
    "g = torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "data_path = \"/Users/magnuswiik/prosjektoppgave_data/Masteroppgave_data/Helfisk_Deteksjonssett/\"\n",
    "\n",
    "# use our dataset and defined transformations\n",
    "dataset = SalmonDataset(data_path)\n",
    "dataset_validation = SalmonDataset(data_path)\n",
    "dataset_test = SalmonDataset(data_path)\n",
    "\n",
    "data_indices = np.arange(0,len(dataset.imgs)*0.2, dtype=np.int16).tolist() # TODO: Fjern *0.2\n",
    "\n",
    "indices_test = random.sample(data_indices, int(len(data_indices)*0.2))\n",
    "data_indices = [idx for idx in data_indices if idx not in indices_test]\n",
    "\n",
    "indices_validation = random.sample(data_indices, int(len(data_indices)*0.2))\n",
    "data_indices = [idx for idx in data_indices if idx not in indices_validation]\n",
    "\n",
    "indices_training = random.sample(data_indices, int(len(data_indices)))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "dataset_training = torch.utils.data.Subset(dataset, indices_training) # 80% for training and validation\n",
    "dataset_validation = torch.utils.data.Subset(dataset_validation, indices_validation)\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices_test) # 20% for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions for training 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "# this class keeps track of the training and validation loss values...\n",
    "# ... and helps to get the average for each epoch as well\n",
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "        \n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "    \n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "        \n",
    "        \n",
    "class SaveBestModel:\n",
    "    \"\"\"\n",
    "    Class to save the best model while training. If the current epoch's \n",
    "    validation loss is less than the previous least less, then save the\n",
    "    model state.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, best_valid_loss=float('inf')\n",
    "    ):\n",
    "        self.best_valid_loss = best_valid_loss\n",
    "        \n",
    "    def __call__(\n",
    "        self, current_valid_loss, \n",
    "        epoch, model, optimizer\n",
    "    ):\n",
    "        if current_valid_loss < self.best_valid_loss:\n",
    "            self.best_valid_loss = current_valid_loss\n",
    "            print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n",
    "            print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
    "            torch.save({\n",
    "                'epoch': epoch+1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, 'outputs/best_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions for training 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    To handle the data loading as different images may have different number \n",
    "    of objects and to handle varying size tensors as well.\n",
    "    \"\"\"\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def save_model(epoch, model, optimizer):\n",
    "    \"\"\"\n",
    "    Function to save the trained model till current epoch, or whenver called\n",
    "    \"\"\"\n",
    "    torch.save({\n",
    "                'epoch': epoch+1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, 'outputs/last_model.pt')\n",
    "    \n",
    "    \n",
    "def save_loss_plot(OUT_DIR, train_loss, val_loss):\n",
    "    figure_1, train_ax = plt.subplots()\n",
    "    figure_2, valid_ax = plt.subplots()\n",
    "    train_ax.plot(train_loss, color='tab:blue')\n",
    "    train_ax.set_xlabel('iterations')\n",
    "    train_ax.set_ylabel('train loss')\n",
    "    valid_ax.plot(val_loss, color='tab:red')\n",
    "    valid_ax.set_xlabel('iterations')\n",
    "    valid_ax.set_ylabel('validation loss')\n",
    "    figure_1.savefig(f\"{OUT_DIR}/train_loss.png\")\n",
    "    figure_2.savefig(f\"{OUT_DIR}/valid_loss.png\")\n",
    "    print('SAVING PLOTS COMPLETE...')\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/magnuswiik/miniconda3/envs/fisketrening/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "|Epoch: 1/1| Loss: 0.3208: 100%|██████████| 22/22 [01:27<00:00,  3.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is saved at:models/model1/model1.pt\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "import utils\n",
    "import os, json\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader_training = torch.utils.data.DataLoader(\n",
    "    dataset_training,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=utils.collate_fn,\n",
    "    generator=g\n",
    ")\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader_validation = torch.utils.data.DataLoader(\n",
    "    dataset_validation,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=utils.collate_fn,\n",
    "    generator=g\n",
    ")\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=utils.collate_fn,\n",
    "    generator=g\n",
    ")\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_detection_model(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(\n",
    "    params,\n",
    "    lr=0.005,\n",
    "    momentum=0.9,\n",
    "    weight_decay=0.0005\n",
    ")\n",
    "\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=3,\n",
    "    gamma=0.1\n",
    ")\n",
    "\n",
    "### Training\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "train_loss_hist = Averager()\n",
    "train_loss_list = []\n",
    "lr_step_sizes = []\n",
    "validation_losses = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # initialize tqdm progress bar\n",
    "    prog_bar = tqdm(data_loader_training, total=len(data_loader_training))\n",
    "    \n",
    "    train_loss_per_epoch = []\n",
    "\n",
    "    for i, data in enumerate(prog_bar):\n",
    "        optimizer.zero_grad()\n",
    "        images, targets = data\n",
    "        \n",
    "        images = list(image.to(device) for image in images)\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        train_loss_per_epoch.append(loss_value)\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # update the loss value beside the progress bar for each iteration\n",
    "        prog_bar.set_description(desc=f\"|Epoch: {epoch+1}/{num_epochs}| Loss: {loss_value:.4f}\")\n",
    "    \n",
    "    validation_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader_validation:\n",
    "            images = [image.to(device) for image in images]\n",
    "            loss_dict = model(images, targets)\n",
    "            loss = sum(loss for loss in loss_dict.values())\n",
    "            validation_loss += loss.item()\n",
    "    \n",
    "    \n",
    "    # Save metrics per epoch\n",
    "    lr_step_sizes.append(lr_scheduler.get_last_lr()[0])\n",
    "    train_loss_list.append(sum(loss for loss in train_loss_per_epoch)/len(train_loss_per_epoch))\n",
    "    train_loss_hist.send(sum(loss for loss in train_loss_per_epoch)/len(train_loss_per_epoch))\n",
    "    validation_loss /= len(data_loader_validation)\n",
    "    validation_losses.append(validation_loss)\n",
    "\n",
    "\n",
    "### SAVING RESULTS\n",
    "\n",
    "MODELPATH = \"models/\" + \"model1/\"\n",
    "\n",
    "if not os.path.exists(MODELPATH):\n",
    "    os.mkdir(MODELPATH)\n",
    "    \n",
    "dict = {'training_loss': train_loss_list, 'lr_step_size': lr_step_sizes, 'validation_losses': validation_losses}\n",
    "df = pd.DataFrame(dict)\n",
    "df.to_csv(MODELPATH + 'metrics.csv', index=False)\n",
    "\n",
    "torch.save(model.state_dict(), MODELPATH + \"model1.pt\")\n",
    "print(\"Model is saved at:\" + MODELPATH + \"model1.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fisketrening",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
